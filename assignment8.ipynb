{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPECT Dataset\n",
    "By:Charu Saxena(A20378393)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA DESCRIPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset describes diagnosing of cardiac Single Proton Emission Computed Tomography (SPECT) images.A SPECT scan of the heart is a noninvasive nuclear imaging test. It uses radioactive tracers that are injected into the blood to produce pictures of your heart. Doctors use SPECT to diagnose coronary artery disease. SPECT can show how well blood is flowing to the heart and how well the heart is working.It helps to find out  a person has coronary artery disease, had a heart attack or at a risk of it or has areas of scar tissue exist.\n",
    "\n",
    "Each of the patients is classified into two categories: normal and abnormal. \n",
    "The database of 267 SPECT image sets (patients) was processed to extract features that summarize the original SPECT images.  \n",
    "As a result, 44 continuous feature pattern was created for each patient.\n",
    "The pattern was further processed to obtain 22 binary feature patterns.\n",
    "The CLIP3 algorithm was used to generate classification rules from these patterns.The CLIP3 algorithm generated rules that were 84.0% accurate (as compared with cardilogists' diagnoses). \n",
    "\n",
    "     Number of Instances: 267\n",
    "     Number of Attributes: 23 (22 binary + 1 binary class)\n",
    "     Attribute Information:\n",
    "   1.  OVERALL_DIAGNOSIS: 0,1 (class attribute, binary)\n",
    "   2.  F1:  0,1 (the partial diagnosis 1, binary)\n",
    "   3.  F2:  0,1 (the partial diagnosis 2, binary)\n",
    "   4.  F3:  0,1 (the partial diagnosis 3, binary)\n",
    "   5.  F4:  0,1 (the partial diagnosis 4, binary)\n",
    "   6.  F5:  0,1 (the partial diagnosis 5, binary)\n",
    "   7.  F6:  0,1 (the partial diagnosis 6, binary)\n",
    "   8.  F7:  0,1 (the partial diagnosis 7, binary)\n",
    "   9.  F8:  0,1 (the partial diagnosis 8, binary)\n",
    "   10. F9:  0,1 (the partial diagnosis 9, binary)\n",
    "   11. F10: 0,1 (the partial diagnosis 10, binary)\n",
    "   12. F11: 0,1 (the partial diagnosis 11, binary)\n",
    "   13. F12: 0,1 (the partial diagnosis 12, binary)\n",
    "   14. F13: 0,1 (the partial diagnosis 13, binary)\n",
    "   15. F14: 0,1 (the partial diagnosis 14, binary)\n",
    "   16. F15: 0,1 (the partial diagnosis 15, binary)\n",
    "   17. F16: 0,1 (the partial diagnosis 16, binary)\n",
    "   18. F17: 0,1 (the partial diagnosis 17, binary)\n",
    "   19. F18: 0,1 (the partial diagnosis 18, binary)\n",
    "   20. F19: 0,1 (the partial diagnosis 19, binary)\n",
    "   21. F20: 0,1 (the partial diagnosis 20, binary)\n",
    "   22. F21: 0,1 (the partial diagnosis 21, binary)\n",
    "   23. F22: 0,1 (the partial diagnosis 22, binary)\n",
    "   \n",
    "   \n",
    "           dataset is divided into:\n",
    "            a) training data (\"SPECT.train\" 80 instances)\n",
    "            b) testing data (\"SPECT.test\" 187 instances)\n",
    "    \n",
    "            Missing Attribute Values: None\n",
    "\n",
    "             Class Distribution:\n",
    "              entire data\n",
    "                Class\t\t# examples\n",
    "                   0\t\t  55\n",
    "                   1\t\t  212\n",
    "               training dataset\n",
    "                 Class\t\t# examples\n",
    "                    0         40\n",
    "                    1\t\t  40\n",
    "               testing dataset\n",
    "                  Class\t\t# examples\n",
    "                    0\t\t  15\n",
    "                    1\t\t  172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv (\"/Users/charusaxena/desktop/assignment2_data/SPECT-train.csv\")\n",
    "test_data = pd.read_csv (\"/Users/charusaxena/desktop/assignment2_data/SPECT-test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = train_data['f23']\n",
    "features = train_data\n",
    "del features['f23']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm = BernoulliNB()\n",
    "model = lm.fit(features,y)\n",
    "y_test = test_data['f23']\n",
    "X_test = test_data\n",
    "del X_test['f23']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = lm.predict_proba(X_test)\n",
    "#predict\n",
    "#print (\"accuracy %2f\" % model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.71428571  0.39285714  0.28571429  0.42857143  0.35714286  0.32142857\n",
      "  0.14285714  0.46428571  0.32142857  0.25        0.32142857  0.28571429\n",
      "  0.46428571  0.57142857  0.28571429  0.14285714  0.35714286  0.21428571\n",
      "  0.25        0.17857143  0.32142857  0.46428571]\n"
     ]
    }
   ],
   "source": [
    "list_of_feature1no_prob = np.exp(lm.feature_log_prob_[0])\n",
    "list_of_feature1yes_prob = np.exp(lm.feature_log_prob_[1])\n",
    "\n",
    "print(list_of_feature1yes_prob)\n",
    "list_of_feature0yes_prob = 1-list_of_feature1yes_prob\n",
    "list_of_feature0no_prob = 1-list_of_feature1no_prob\n",
    "list_of_feature0yes_prob\n",
    "\n",
    "class_prob = np.exp(lm.class_log_prior_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evidence/odds for both feature value 1 and 0 thus taking p(x=1/yes)/p(x=1/no) and p(x=0/yes)/p(x=0/no)\n",
    "odds_feature0 = [a/b for a,b in zip(list_of_feature0yes_prob,list_of_feature0no_prob)]\n",
    "odds_feature1 = [a/b for a,b in zip(list_of_feature1yes_prob,list_of_feature1no_prob)]\n",
    "\n",
    "ods_class_prior = class_prob[1]/class_prob[0]\n",
    "#adding class prior to both feature value 1 and feature value 0 odds/evidence p(+/-)\n",
    "\n",
    "odds_feature0.append(ods_class_prior)#p(x=0/yes)/p(x=0/no)... p(+/-)\n",
    "odds_feature1.append(ods_class_prior)#p(x=1/yes)/p(x=1/no).... p(+/-)\n",
    "#odds_feature1,odds_feature0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Total_evidence(object,class_prior):\n",
    "    positive_evidence=[]\n",
    "    negative_evidence=[]\n",
    "    if np.log(class_prior)>0:\n",
    "        positive_evidence.append(np.log(class_prior)) # add log(p(yes)/p(no)) if p(yes)/p(no) >1\n",
    "    else:\n",
    "        negative_evidence.append(np.log(class_prior))\n",
    "    for i,val in enumerate(object):\n",
    "        if val==1:\n",
    "            if np.log(odds_feature1[i])>0:\n",
    "                positive_evidence.append(np.log(odds_feature1[i]))#add log(p(x=0/yes)/p(x=0/no)) if p(x=0/yes)/p(x=0/no)>1\n",
    "            else:\n",
    "                negative_evidence.append(np.log(odds_feature1[i]))\n",
    "        else:\n",
    "            if np.log(odds_feature0[i])>0:\n",
    "                positive_evidence.append(np.log(odds_feature0[i]))\n",
    "            else:\n",
    "                negative_evidence.append(np.log(odds_feature0[i]))\n",
    "    \n",
    "    total_positive_evidence=sum(positive_evidence)\n",
    "    total_negative_evidence=sum(negative_evidence)\n",
    "    return total_positive_evidence, total_negative_evidence\n",
    "\n",
    "def evidence(object,class_prior):##also check if there is any previous value greater than the current postitve for a \n",
    "    #instnce\n",
    "    positive_evidence=[]\n",
    "    negative_evidence=[]\n",
    "    \n",
    "    for i,val in enumerate(object):\n",
    "        if val==1:\n",
    "            if np.log(odds_feature1[i])>0:\n",
    "                positive_evidence.append(np.log(odds_feature1[i]))#add log(p(x=0/yes)/p(x=0/no)) if p(x=0/yes)/p(x=0/no)>1\n",
    "            else:\n",
    "                negative_evidence.append(np.log(odds_feature1[i]))\n",
    "        else:\n",
    "            if np.log(odds_feature0[i])>0:\n",
    "                positive_evidence.append(np.log(odds_feature0[i]))\n",
    "            else:\n",
    "                negative_evidence.append(np.log(odds_feature0[i]))\n",
    "   \n",
    "    return positive_evidence, negative_evidence\n",
    " \n",
    "    \n",
    "def top_features(object,pos,neg):\n",
    "    evidence=[]\n",
    "    for i,val in enumerate(object):\n",
    "        if val==1:\n",
    "            \n",
    "                evidence.append(np.log(odds_feature1[i]))#add log(p(x=0/yes)/p(x=0/no)) if p(x=0/yes)/p(x=0/no)>1\n",
    "        else:\n",
    "                evidence.append(np.log(odds_feature0[i]))\n",
    "    \n",
    "    evidence1=evidence\n",
    "    Top_neg_index = np.argsort(evidence)\n",
    "    \n",
    "    #Top_pos_index = evidence.argsort()[::-1][:n]\n",
    "    if neg>3:\n",
    "        idx = 3\n",
    "    else:\n",
    "        idx=neg\n",
    "        \n",
    "    if pos>3:\n",
    "        idx1 = 3\n",
    "    else:\n",
    "        idx1=pos  \n",
    "    return Top_neg_index[:idx],Top_neg_index[-idx1:],evidence1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. To find most positive object wrt to prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    a) the total positive log-evidence,\n",
    "    b) the total negative log-evidence, \n",
    "    c) probability distribution, \n",
    "    d) top 3 features values that contribute most to the positive evidence, \n",
    "    e) top 3 feature values that contribute the most to the negative evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most positive object wrt prob:\n",
      " f1     1\n",
      "f2     1\n",
      "f3     1\n",
      "f4     1\n",
      "f5     1\n",
      "f6     1\n",
      "f7     1\n",
      "f8     1\n",
      "f9     1\n",
      "f10    1\n",
      "f11    1\n",
      "f12    0\n",
      "f13    1\n",
      "f14    1\n",
      "f15    1\n",
      "f16    1\n",
      "f17    1\n",
      "f18    1\n",
      "f19    1\n",
      "f20    1\n",
      "f21    1\n",
      "f22    1\n",
      "Name: 158, dtype: int64\n",
      "\n",
      "total positive evidence: 14.4112139432\n",
      "\n",
      "total negative evidence: -1.05297100771\n",
      "\n",
      "class distributuion [  1.57974911e-06   9.99998420e-01]\n",
      "\n",
      "\n",
      "top negative features are: Index(['f20', 'f12'], dtype='object') \n",
      "top positive features Index(['f22', 'f17', 'f19'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#print(\"most positive object wrt to nprob is:\",yesclass.index(max(yesclass)),\"and value is :\",max(yesclass))\n",
    "most_postive_object = test_data.iloc[(np.argmax(predict[:,1]))]\n",
    "print(\"most positive object wrt prob:\\n\",most_postive_object )\n",
    "\n",
    "a = Total_evidence(most_postive_object,ods_class_prior)\n",
    "\n",
    "print(\"\\ntotal positive evidence:\",a[0])\n",
    "print(\"\\ntotal negative evidence:\",a[1])\n",
    "print(\"\\nclass distributuion\",(predict[np.argmax(predict[:,1])]))\n",
    "\n",
    "\n",
    "pos_evi=evidence(most_postive_object,ods_class_prior)[0]\n",
    "neg_evi=evidence(most_postive_object,ods_class_prior)[1]\n",
    "\n",
    "#print(\"\\n top features\",top_features(most_postive_object,len(pos_evi),len(neg_evi)))\n",
    "top_features(most_postive_object,len(pos_evi),len(neg_evi))\n",
    "indexNeg= top_features(most_postive_object,len(pos_evi),len(neg_evi))[0]\n",
    "indexPos = top_features(most_postive_object,len(pos_evi),len(neg_evi))[1]\n",
    "\n",
    "columns = X_test.columns\n",
    "\n",
    "print(\"\\n\\ntop negative features are:\",columns[indexNeg],\"\\ntop positive features\",columns[indexPos])\n",
    "\n",
    "#print(\"\\n\\n\",evidence(most_postive_object,ods_class_prior))\n",
    "#top_features(most_postive_object,len(pos_evi),len(neg_evi))[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yes-1 and no-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.To find most negative object wrt probabilties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    a) the total positive log-evidence,\n",
    "    b) the total negative log-evidence, \n",
    "    c) probability distribution, \n",
    "    d) top 3 features values that contribute most to the positive evidence, \n",
    "    e) top 3 feature values that contribute the most to the negative evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most negative object wrt prob:\n",
      " f1     0\n",
      "f2     0\n",
      "f3     0\n",
      "f4     0\n",
      "f5     0\n",
      "f6     0\n",
      "f7     0\n",
      "f8     0\n",
      "f9     0\n",
      "f10    0\n",
      "f11    0\n",
      "f12    0\n",
      "f13    0\n",
      "f14    0\n",
      "f15    0\n",
      "f16    0\n",
      "f17    0\n",
      "f18    0\n",
      "f19    0\n",
      "f20    0\n",
      "f21    0\n",
      "f22    0\n",
      "Name: 172, dtype: int64\n",
      "\n",
      "total positive evidence: 0.0444517625708\n",
      "\n",
      "total negative evidence: -5.79641857806\n",
      "\n",
      "class distributuion [ 0.99683353  0.00316647]\n",
      "\n",
      "\n",
      "top negative features are: Index(['f1', 'f14', 'f22'], dtype='object') \n",
      "top positive features Index(['f20'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "most_negative_object = test_data.iloc[(np.argmax(predict[:,0]))]\n",
    "\n",
    "print(\"most negative object wrt prob:\\n\",most_negative_object )\n",
    "for i,val in enumerate(most_negative_object):\n",
    "    if val==1:\n",
    "            print(i)\n",
    "\n",
    "b= Total_evidence(most_negative_object,ods_class_prior)\n",
    "\n",
    "print(\"\\ntotal positive evidence:\",b[0])\n",
    "print(\"\\ntotal negative evidence:\",b[1])\n",
    "print(\"\\nclass distributuion\",(predict[np.argmax(predict[:,0])]))\n",
    "\n",
    "\n",
    "pos_evi=evidence(most_negative_object,ods_class_prior)[0]\n",
    "neg_evi=evidence(most_negative_object,ods_class_prior)[1]\n",
    "\n",
    "#print(\"\\n top features\",top_features(most_postive_object,len(pos_evi),len(neg_evi)))\n",
    "top_features(most_negative_object,len(pos_evi),len(neg_evi))\n",
    "indexNeg= top_features(most_negative_object,len(pos_evi),len(neg_evi))[0]\n",
    "indexPos = top_features(most_negative_object,len(pos_evi),len(neg_evi))[1]\n",
    "\n",
    "columns = X_test.columns\n",
    "\n",
    "print(\"\\n\\ntop negative features are:\",columns[indexNeg],\"\\ntop positive features\",columns[indexPos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. To find object with highest positive evidence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    a) the total positive log-evidence,\n",
    "    b) the total negative log-evidence, \n",
    "    c) probability distribution, \n",
    "    d) top 3 features values that contribute most to the positive evidence, \n",
    "    e) top 3 feature values that contribute the most to the negative evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object with highest positive evidence:\n",
      " f1     1\n",
      "f2     1\n",
      "f3     0\n",
      "f4     0\n",
      "f5     1\n",
      "f6     1\n",
      "f7     0\n",
      "f8     0\n",
      "f9     0\n",
      "f10    1\n",
      "f11    1\n",
      "f12    0\n",
      "f13    0\n",
      "f14    0\n",
      "f15    1\n",
      "f16    1\n",
      "f17    1\n",
      "f18    0\n",
      "f19    0\n",
      "f20    1\n",
      "f21    1\n",
      "f22    0\n",
      "Name: 0, dtype: int64\n",
      "\n",
      "total positive evidence: 4.86266866212\n",
      "\n",
      "total negative evidence: -4.01555564933\n",
      "\n",
      "class distributuion [ 0.30003882  0.69996118]\n",
      "\n",
      "\n",
      "top negative features are: Index(['f14', 'f22', 'f13'], dtype='object') \n",
      "top positive features Index(['f16', 'f5', 'f17'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "testdata = X_test.as_matrix()\n",
    "\n",
    "positive_lg_evidence=[]\n",
    "negative_lg_evidence=[]\n",
    "lg_evidence=[]\n",
    "for i in testdata:\n",
    "    \n",
    "    positive_lg_evidence,negative_lg_evidence=evidence(i,ods_class_prior)\n",
    "\n",
    "object_highest_PosEvi = test_data.iloc[(np.argmax(positive_lg_evidence))]\n",
    "print(\"object with highest positive evidence:\\n\",object_highest_PosEvi )\n",
    "\n",
    "\n",
    "c= Total_evidence(object_highest_PosEvi,ods_class_prior)\n",
    "print(\"\\ntotal positive evidence:\",c[0])\n",
    "print(\"\\ntotal negative evidence:\",c[1])\n",
    "print(\"\\nclass distributuion\",(predict[np.argmax(positive_lg_evidence)]))\n",
    "\n",
    "pos_evi=evidence(object_highest_PosEvi,ods_class_prior)[0]\n",
    "neg_evi=evidence(object_highest_PosEvi,ods_class_prior)[1]\n",
    "\n",
    "#print(\"\\n top features\",top_features(most_postive_object,len(pos_evi),len(neg_evi)))\n",
    "top_features(object_highest_PosEvi,len(pos_evi),len(neg_evi))\n",
    "indexNeg= top_features(object_highest_PosEvi,len(pos_evi),len(neg_evi))[0]\n",
    "indexPos = top_features(object_highest_PosEvi,len(pos_evi),len(neg_evi))[1]\n",
    "\n",
    "columns = X_test.columns\n",
    "\n",
    "print(\"\\n\\ntop negative features are:\",columns[indexNeg],\"\\ntop positive features\",columns[indexPos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. To find object with highest negative evidence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    a) the total positive log-evidence,\n",
    "    b) the total negative log-evidence, \n",
    "    c) probability distribution, \n",
    "    d) top 3 features values that contribute most to the positive evidence, \n",
    "    e) top 3 feature values that contribute the most to the negative evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object with highest Negative evidence:\n",
      " f1     1\n",
      "f2     1\n",
      "f3     0\n",
      "f4     0\n",
      "f5     1\n",
      "f6     0\n",
      "f7     0\n",
      "f8     1\n",
      "f9     1\n",
      "f10    1\n",
      "f11    1\n",
      "f12    0\n",
      "f13    1\n",
      "f14    1\n",
      "f15    1\n",
      "f16    0\n",
      "f17    1\n",
      "f18    0\n",
      "f19    0\n",
      "f20    0\n",
      "f21    1\n",
      "f22    0\n",
      "Name: 6, dtype: int64\n",
      "\n",
      "total positive evidence: 7.18219836599 \t\t total negative evidence: -2.42725635682\n",
      "\n",
      "class distributuion [ 0.00853556  0.99146444]\n",
      "\n",
      "\n",
      "top negative features are: Index(['f22', 'f4', 'f19'], dtype='object') \n",
      "top positive features Index(['f8', 'f13', 'f17'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "testdata = X_test.as_matrix()\n",
    "\n",
    "positive_lg_evidence=[]\n",
    "negative_lg_evidence=[]\n",
    "lg_evidence=[]\n",
    "for i in testdata:\n",
    "    \n",
    "    positive_lg_evidence,negative_lg_evidence=evidence(i,ods_class_prior)\n",
    "\n",
    "object_highest_NegEvi = test_data.iloc[(np.argmax(negative_lg_evidence))]\n",
    "print(\"object with highest Negative evidence:\\n\",object_highest_NegEvi )\n",
    "d= Total_evidence(object_highest_NegEvi,ods_class_prior)\n",
    "print(\"\\ntotal positive evidence:\",d[0],\"\\t\\t total negative evidence:\",d[1])\n",
    "print(\"\\nclass distributuion\",(predict[np.argmax(negative_lg_evidence)]))\n",
    "\n",
    "\n",
    "\n",
    "pos_evi=evidence(object_highest_NegEvi,ods_class_prior)[0]\n",
    "neg_evi=evidence(object_highest_NegEvi,ods_class_prior)[1]\n",
    "\n",
    "#print(\"\\n top features\",top_features(most_postive_object,len(pos_evi),len(neg_evi)))\n",
    "top_features(object_highest_NegEvi,len(pos_evi),len(neg_evi))\n",
    "indexNeg= top_features(object_highest_NegEvi,len(pos_evi),len(neg_evi))[0]\n",
    "indexPos = top_features(object_highest_NegEvi,len(pos_evi),len(neg_evi))[1]\n",
    "\n",
    "columns = X_test.columns\n",
    "\n",
    "print(\"\\n\\ntop negative features are:\",columns[indexNeg],\"\\ntop positive features\",columns[indexPos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. To find object which is most uncertain  #fullon neend me kiya check again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    a) the total positive log-evidence,\n",
    "    b) the total negative log-evidence, \n",
    "    c) probability distribution, \n",
    "    d) top 3 features values that contribute most to the positive evidence, \n",
    "    e) top 3 feature values that contribute the most to the negative evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object which most uncertain :\n",
      " f1     1\n",
      "f2     0\n",
      "f3     0\n",
      "f4     1\n",
      "f5     0\n",
      "f6     1\n",
      "f7     0\n",
      "f8     0\n",
      "f9     0\n",
      "f10    0\n",
      "f11    1\n",
      "f12    0\n",
      "f13    0\n",
      "f14    1\n",
      "f15    0\n",
      "f16    0\n",
      "f17    0\n",
      "f18    0\n",
      "f19    0\n",
      "f20    0\n",
      "f21    0\n",
      "f22    1\n",
      "Name: 73, dtype: int64\n",
      "\n",
      "total positive evidence: 3.6027223399 \t\t total negative evidence: -3.59483199074\n",
      "\n",
      "class distributuion [ 0.30003882  0.69996118]\n",
      "[0.59783700075562063, 0.78015855754957564, 0.057158413839949046, 0.11778303565638425, 0.82667857318446847, 0.044451762570833588, 1.1786549963416468] [-0.057158413839949108, -0.2029408439966903, -0.26662866325394874, -1.1102230246251565e-16, -0.42744401482693978, -0.075985906977922221, -0.090971778205726633, -0.1397619423751586, -0.44895022004790353, -0.1397619423751586, -0.080042707673536495, -0.32850406697203627, -0.16705408466316621, -0.26966356694910271, -0.16907633004393413]\n",
      "\n",
      "\n",
      "top negative features are: Index(['f13', 'f8', 'f17'], dtype='object') \n",
      "top positive features Index(['f4', 'f14', 'f22'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# fully neend me kiya not sure kya krri hun\n",
    "\n",
    "for i in predict:\n",
    "    value= np.argmin(np.absolute([x[0]-x[1] for x in predict]))\n",
    "\n",
    "\n",
    "print(\"object which most uncertain :\\n\", X_test.iloc[value])\n",
    "e = Total_evidence(X_test.iloc[value],ods_class_prior)\n",
    "print(\"\\ntotal positive evidence:\",e[0],\"\\t\\t total negative evidence:\",e[1])\n",
    "print(\"\\nclass distributuion\",(predict[np.argmax(value)])) ### am not sure kya krri me\n",
    "\n",
    "\n",
    "m,n=evidence(X_test.iloc[value],ods_class_prior)\n",
    "print(m,n)\n",
    "\n",
    "pos_evi=evidence(X_test.iloc[value],ods_class_prior)[0]\n",
    "neg_evi=evidence(X_test.iloc[value],ods_class_prior)[1]\n",
    "\n",
    "#print(\"\\n top features\",top_features(most_postive_object,len(pos_evi),len(neg_evi)))\n",
    "top_features(X_test.iloc[value],len(pos_evi),len(neg_evi))\n",
    "indexNeg= top_features(X_test.iloc[value],len(pos_evi),len(neg_evi))[0]\n",
    "indexPos = top_features(X_test.iloc[value],len(pos_evi),len(neg_evi))[1]\n",
    "\n",
    "columns = X_test.columns\n",
    "\n",
    "print(\"\\n\\ntop negative features are:\",columns[indexNeg],\"\\ntop positive features\",columns[indexPos])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
